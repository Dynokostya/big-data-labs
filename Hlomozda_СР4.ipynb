{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jLE0nxT_P19"
      },
      "source": [
        "\n",
        "<center><font size=\"6\"><b>Комп'ютерний практикум 4.\n",
        "\n",
        " Основи PySpark: обробка даних DataFrame</b></font></center>\n",
        "\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb_WlwqB8hrg"
      },
      "source": [
        "<center><img src=\"https://media.licdn.com/dms/image/v2/C4E12AQEb6oxAxtYD-Q/article-cover_image-shrink_600_2000/article-cover_image-shrink_600_2000/0/1620420835464?e=2147483647&v=beta&t=EWVtIY_UWl1XNucKkGMrn6ooEuwOBaQ313Z1TarbZgk\" width=\"400\"></center>\n",
        "\n",
        "\n",
        "**PySpark** — це інтерфейс для використання **Apache Spark** з мовою програмування **Python**.\n",
        "\n",
        "**Apache Spark** — це потужний фреймворк для розподіленої обробки пакетних даних у кластері. PySpark надає Python API для інтеграції цих можливостей.\n",
        "\n",
        "### Основні особливості PySpark:\n",
        "\n",
        "1. **Обробка великих даних**: PySpark дозволяє обробляти великі набори даних розподілено на кількох вузлах у кластері, використовуючи потужні обчислювальні кластери, що пришвидшує виконання операцій.\n",
        "   \n",
        "2. **API для Python**: PySpark надає простий API для роботи з даними, який включає операції над RDD (Resilient Distributed Datasets), DataFrames, SQL, та Machine Learning.\n",
        "\n",
        "3. **Інструменти для обробки даних у реальному часі**: PySpark підтримує обробку даних у потоковому режимі через модуль **Spark Streaming**.\n",
        "\n",
        "4. **Машинне навчання**: PySpark включає бібліотеку для машинного навчання (MLlib), що дозволяє виконувати завдання класифікації, кластеризації, регресії та зниження розмірності.\n",
        "\n",
        "### Використання:\n",
        "- PySpark активно використовується для ___ETL-процесів___, де необхідно обробляти великі обсяги даних.\n",
        "- Використовується для ___аналітики___ та ___машинного навчання___ на масштабних даних.\n",
        "- Є популярним інструментом в середовищах ___Big Data___, таких як Amazon EMR, Google Dataproc та Microsoft Azure HDInsight.\n",
        "\n",
        "PySpark робить можливим використання простоти Python для роботи з великими даними в Apache Spark, що забезпечує гнучкість і масштабованість обчислень.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDMPJL418PuJ"
      },
      "source": [
        "> __Spark SQL__ - це модуль Spark для обробки структурованих даних. Він призначений для запитів до структурованих даних у програмах Spark, використовуючи або SQL, або знайомий API DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h2YOKaRS9BYm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in ./.venv/lib/python3.12/site-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in ./.venv/lib/python3.12/site-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: findspark in ./.venv/lib/python3.12/site-packages (2.0.1)\n",
            "Requirement already satisfied: pyarrow in ./.venv/lib/python3.12/site-packages (17.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in ./.venv/lib/python3.12/site-packages (from pyarrow) (2.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Встановлюємо необхідні пакети\n",
        "! pip install pyspark\n",
        "! pip install findspark\n",
        "! pip install pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MUWZYDVj8eT0"
      },
      "outputs": [],
      "source": [
        "# імпорт допоміжної бібліотеки для організації правильного шляху до Spark\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYU6BmAc-eel"
      },
      "source": [
        "Для роботи з **Apache Spark** у Python через бібліотеку **PySpark** необхідно імпортувати наступні модулі\n",
        "\n",
        "**`SparkContext`**:\n",
        "   - Це об'єкт, який є точкою входу в функціональні можливості Spark. Він представляє з'єднання програми зі **Spark кластером** (або локальним екземпляром Spark).\n",
        "   - `SparkContext` використовується для створення **RDD (Resilient Distributed Dataset)**, а також для взаємодії зі Spark API. Він керує роботою і розподілом завдань між вузлами кластера.\n",
        "   - У сучасних версіях Spark, `SparkContext` зазвичай створюється автоматично всередині **`SparkSession`**, тому його часто не потрібно ініціалізувати окремо, якщо використовується `SparkSession`.\n",
        "\n",
        "**`SparkConf`**:\n",
        "   - `SparkConf` дозволяє налаштувати параметри роботи Spark-додатка, такі як ім'я додатка, кількість ядер, кількість пам'яті тощо. Об'єкт `SparkConf` при створенні передається `SparkContext` або `SparkSession`, щоб визначити, як додаток повинен працювати.\n",
        "   - Це спосіб налаштувати специфічні параметри роботи Spark на рівні додатку.\n",
        "\n",
        "**`SparkSession`**:\n",
        "   - Це вищий рівень абстракції, який був введений у Spark 2.0. `SparkSession` об'єднує всі попередні функціональні можливості **SparkContext**, **SQLContext** і **HiveContext** в одному об'єкті. Це єдина точка входу для роботи з різними компонентами Spark (RDD, DataFrame, SQL-запити).\n",
        "   - `SparkSession` також використовується для роботи з **DataFrame**, SQL-запитами та інтеграції з іншими компонентами, такими як **Spark SQL**, **Streaming**, **MLlib** та **GraphX**.\n",
        "> **Основні можливості `SparkSession`**:\n",
        "   - Робота з `DataFrame` та SQL-запитами.\n",
        "   - Використання `Spark SQL` для взаємодії з даними у стилі SQL.\n",
        "   - Створення і керування RDD.\n",
        "   - Інтеграція з різними джерелами даних (HDFS, Cassandra, S3 та ін.).\n",
        "\n",
        "### Коли використовувати?\n",
        "\n",
        "- **`SparkContext`**: Якщо ви працюєте зі **старими версіями Spark** або хочете безпосередньо взаємодіяти з RDD.\n",
        "- **`SparkSession`**: У більшості сучасних додатків, починаючи з **Spark 2.0**, рекомендовано використовувати **SparkSession** для створення та керування всіма обчислювальними компонентами (DataFrame, SQL-запити та RDD).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "s07FBcRA94Zq"
      },
      "outputs": [],
      "source": [
        "#імпорт бібліотек та модулів\n",
        "\n",
        "import pandas as pd\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97rJQSzw61Hk"
      },
      "source": [
        "## Створення Spark сесії\n",
        "\n",
        "- `.builder` - це спосіб налаштувати і створити `SparkSession`, який є головним об'єктом у сучасних версіях Spark\n",
        "- `.appName(\"name\")` задає ім'я програми Spark для ідентифікації додатку під час його роботи в кластері або в локальному середовищі\n",
        "- `.config()` -  метод дозволяє передавати додаткові конфігураційні параметри для Spark\n",
        "- `.getOrCreate()` - метод або створює новий об'єкт `SparkSession`, або повертає вже існуючий, якщо такий є."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tgYEg7Pl7g7A"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/10/26 13:47:37 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
            "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
            "java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n",
            "java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\n",
            "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\n",
            "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
            "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "py4j.Gateway.invoke(Gateway.java:238)\n",
            "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
            "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
            "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "java.base/java.lang.Thread.run(Thread.java:1575)\n"
          ]
        },
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:347)\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:577)\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2416)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2416)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:329)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1575)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Створення об'єкту spark context class (не обов'язково)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m SPARK_LOCAL_IP\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m127.0.0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Створення spark session\u001b[39;00m\n\u001b[1;32m      7\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession \\\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy_Spark_basic\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.some.config.option\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msome-value\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
            "File \u001b[0;32m~/Documents/Projects/big-data-labs/.venv/lib/python3.12/site-packages/pyspark/context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
            "File \u001b[0;32m~/Documents/Projects/big-data-labs/.venv/lib/python3.12/site-packages/pyspark/context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
            "File \u001b[0;32m~/Documents/Projects/big-data-labs/.venv/lib/python3.12/site-packages/pyspark/context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Documents/Projects/big-data-labs/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m~/Documents/Projects/big-data-labs/.venv/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:347)\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:577)\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2416)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2416)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:329)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1575)\n"
          ]
        }
      ],
      "source": [
        "# Створення об'єкту spark context class (не обов'язково)\n",
        "SPARK_LOCAL_IP=\"127.0.0.1\"\n",
        "sc = SparkContext()\n",
        "\n",
        "# Створення spark session\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"My_Spark_basic\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PwHgEeqZE_km"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'spark' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Для роботи з датафреймами необхідно переконатися, що екземпляр сеансу spark створено.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mspark\u001b[49m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
          ]
        }
      ],
      "source": [
        "# Для роботи з датафреймами необхідно переконатися, що екземпляр сеансу spark створено.\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHZ9JQ5w-RKf"
      },
      "source": [
        "## Завантаження даних\n",
        "\n",
        "> Для завантаження даних спочатку імпортується CSV-файл у таблицю даних `Pandas`, а потім передається в таблицю даних `Spark`\n",
        "\n",
        "Для створення фрейму даних `Spark` завантажуємо зовнішній фрейм даних, який називається `mtcars`. Цей фрейм даних містить 32 спостереження та 11 змінних:\n",
        "\n",
        "| colIndex | colName | units/description |\n",
        "| :---: | :--- | :--- |\n",
        "|[, 1] | mpg |Miles per gallon  |\n",
        "|[, 2] | cyl | Number of cylinders  |\n",
        "|[, 3] | disp | Displacement (cu.in.) |  \n",
        "|[, 4] | hp  | Gross horsepower  |\n",
        "|[, 5] | drat | Rear axle ratio  |\n",
        "|[, 6] | wt | Weight (lb/1000)  |\n",
        "|[, 7] | qsec | 1/4 mile time  |\n",
        "|[, 8] | vs  | V/S  |\n",
        "|[, 9] | am | Transmission (0 = automatic, 1 = manual)  |\n",
        "|[,10] | gear | Number of forward gears  |\n",
        "|[,11] | carb | Number of carburetors |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8r6bYJSB-klj"
      },
      "outputs": [],
      "source": [
        "# Використаємо `read_csv` з pandas для завантаження датасету\n",
        "mtcars = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/mtcars.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pa0WnmD_GY8U"
      },
      "outputs": [],
      "source": [
        "# Перегляд датасету\n",
        "mtcars.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2qOe-SuGkEF"
      },
      "outputs": [],
      "source": [
        "# переіменуємо перший стовпчик\n",
        "mtcars.rename( columns={'Unnamed: 0':'name'}, inplace=True )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIzFPdPLG3OY"
      },
      "source": [
        "> Функція `createDataFrame` завантажує дані в spark dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vz7DY4sAHK8j"
      },
      "outputs": [],
      "source": [
        "sdf = spark.createDataFrame(mtcars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-A3cMZcHhIk"
      },
      "source": [
        "> Метод **`printSchema()`** використовується для виведення **схеми** (структури) **DataFrame** у Spark.\n",
        "\n",
        "**Схема** — це структура **DataFrame**, яка визначає назви стовпців і типи даних у цих стовпцях (наприклад, `StringType`, `IntegerType`, `DoubleType` і т.д.). Ця структура дозволяє знати, які дані містить DataFrame та в якому форматі вони зберігаються.\n",
        "\n",
        "Використовується щоб перевірити структуру даних у DataFrame перед виконанням операцій та для верифікації того, що типи даних і назви стовпців вірні, особливо якщо дані із зовнішніх джерел (файли CSV, бази даних тощо)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvB5n6RxHVEg"
      },
      "outputs": [],
      "source": [
        "sdf.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdvmZoL5Ik_F"
      },
      "source": [
        "> Функція `withColumnRenamed()` перейменовує існуючі назви стовпців.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF3Qa3loIqz9"
      },
      "outputs": [],
      "source": [
        "#Переіменовуємо назву стовпця «vs» на «versus» і зберігаємо зміни в новий DataFrame  «sdf_new».\n",
        "sdf_new = sdf.withColumnRenamed(\"vs\", \"versus\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkgb7ErcJGH6"
      },
      "outputs": [],
      "source": [
        "sdf_new.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot3d7m4ZJUhc"
      },
      "source": [
        "## Створення тимчасової таблиці (Table View/табличне представлення)\n",
        "Створення табличного представлення в Spark SQL необхідне для програмного запуску SQL запитів для DataFrame. Представлення - це тимчасова таблиця для виконання SQL запитів, яке забезпечує локальну область видимості в межах поточного сеансу Spark за допомогою функції `createTempView()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIz1UJ52Jvqt"
      },
      "outputs": [],
      "source": [
        "sdf.createTempView(\"cars\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "657E_NTHM0yL"
      },
      "source": [
        "У `PySpark` для роботи з `DataFrame` часто використовують методи, схожі на SQL-запити.\n",
        "\n",
        "__<center>Таблиця основних SQL-запитів та їх еквівалентів у PySpark:</center>__\n",
        "\n",
        "| **SQL Запит**               | **PySpark (DataFrame API)**                                              | **Опис**                                      |\n",
        "|-----------------------------|---------------------------------------------------------------------------|-----------------------------------------------|\n",
        "| **SELECT**                   | `df.select(\"column1\", \"column2\")`                                         | Вибір конкретних колонок                     |\n",
        "| **WHERE**                    | `df.filter(df[\"column\"] == value)`                                        | Фільтрація рядків за умовою                   |\n",
        "| **AND / OR**                 | `df.filter((df[\"col1\"] == val1) & (df[\"col2\"] == val2))`                  | Використання логічних операторів              |\n",
        "| **ORDER BY**                 | `df.orderBy(\"column\", ascending=False)`                                   | Сортування за колонкою                       |\n",
        "| **GROUP BY**                 | `df.groupBy(\"column\").agg({\"column\": \"sum\"})`                             | Групування даних і агрегація                  |\n",
        "| **HAVING**                   | `df.groupBy(\"column\").agg({\"column\": \"sum\"}).filter(\"sum(column) > value\")` | Фільтрація результатів після групування       |\n",
        "| **JOIN**                     | `df1.join(df2, df1[\"id\"] == df2[\"id\"], \"inner\")`                          | Об'єднання двох DataFrame                     |\n",
        "| **LIMIT**                    | `df.limit(10)`                                                           | Обмеження кількості рядків                   |\n",
        "| **DISTINCT**                 | `df.select(\"column\").distinct()`                                          | Вибір унікальних значень                     |\n",
        "| **COUNT**                    | `df.count()`                                                             | Підрахунок кількості рядків                  |\n",
        "| **SUM**                      | `df.groupBy().sum(\"column\")`                                              | Підсумовування значень у колонці             |\n",
        "| **AVG (Середнє значення)**   | `df.groupBy().avg(\"column\")`                                              | Обчислення середнього значення               |\n",
        "| **MAX (Максимум)**           | `df.groupBy().max(\"column\")`                                              | Визначення максимального значення            |\n",
        "| **MIN (Мінімум)**            | `df.groupBy().min(\"column\")`                                              | Визначення мінімального значення             |\n",
        "| **WITH ALIAS (Псевдоніми)**  | `df.select(df[\"column\"].alias(\"new_name\"))`                               | Присвоєння псевдонімів для колонок           |\n",
        "| **INNER JOIN**               | `df1.join(df2, df1[\"id\"] == df2[\"id\"], \"inner\")`                          | Внутрішнє об'єднання                         |\n",
        "| **LEFT JOIN**                | `df1.join(df2, df1[\"id\"] == df2[\"id\"], \"left\")`                           | Ліве об'єднання                              |\n",
        "| **RIGHT JOIN**               | `df1.join(df2, df1[\"id\"] == df2[\"id\"], \"right\")`                          | Праве об'єднання                             |\n",
        "| **FULL OUTER JOIN**          | `df1.join(df2, df1[\"id\"] == df2[\"id\"], \"outer\")`                          | Повне зовнішнє об'єднання                    |\n",
        "| **UNION**                    | `df1.union(df2)`                                                         | Об'єднання двох DataFrame (без дублікатів)   |\n",
        "| **UNION ALL**                | `df1.unionAll(df2)`                                                      | Об'єднання двох DataFrame (з дублікатами)    |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOv5NibCXaso"
      },
      "source": [
        "> `spark.sql` — це метод у **PySpark**, який дозволяє виконувати **SQL-запити** безпосередньо на **DataFrame**. Він використовується для інтеграції SQL з PySpark, дозволяючи працювати з великими даними у знайомому форматі SQL-запитів.\n",
        "\n",
        "1. **Створення тимчасової таблиці**: Це дозволяє звертатися до цього DataFrame в SQL-запитах, як до таблиці в базі даних.\n",
        "   \n",
        "2. **Виконання SQL-запитів**: Після створення тимчасової таблиці можна виконувати SQL-запити з використанням методу `spark.sql()`. Цей метод повертає результат як новий DataFrame.\n",
        "\n",
        "*Синтаксис*\n",
        "```python\n",
        "spark.sql(\"SQL-запит\")\n",
        "```\n",
        "### Переваги використання `spark.sql`:\n",
        "1. **Знайомий синтаксис**: SQL-запити.\n",
        "2. **Масштабованість**: SQL-запити, виконані через `spark.sql`, можуть працювати з великими наборами даних, розподіленими в кластері.\n",
        "3. **Гнучкість**: Можна комбінувати SQL із методами DataFrame API для виконання складних обчислень.\n",
        "\n",
        "`spark.sql` — це потужний інструмент у PySpark, який дозволяє використовувати SQL-запити для обробки великих даних. Він надає гнучкість у використанні стандартного SQL-синтаксису для вибірок, агрегацій та інших операцій на великих наборах даних, що зберігаються в Spark DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHxOBl9RKfxg"
      },
      "outputs": [],
      "source": [
        "# Вивід всієї таблиці даних\n",
        "spark.sql(\"SELECT * FROM cars\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBdMUJ3WKiT5"
      },
      "outputs": [],
      "source": [
        "# Вибір змінної mpg\n",
        "spark.sql(\"SELECT mpg FROM cars\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-k-H4pYKkyP"
      },
      "outputs": [],
      "source": [
        "# Запит для визначення автомобілів з великим пробігом і малою кількістю циліндрів\n",
        "spark.sql(\"SELECT * FROM cars where mpg > 15 AND cyl < 6\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGXSsbhXKnNs"
      },
      "outputs": [],
      "source": [
        "# Авто, які мають витрати палива менше 15 миль на галон\n",
        "sdf.where(sdf['mpg'] < 15).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24fAK9HhKpdn"
      },
      "outputs": [],
      "source": [
        "# Агрегація даних та групування за циліндрами\n",
        "spark.sql(\"SELECT count(*), cyl from cars GROUP BY cyl\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLiC2TVkPc7b"
      },
      "source": [
        "## Створення Pandas UDF\n",
        "\n",
        "**Pandas UDF (User Defined Functions)** в **PySpark** — це користувацькі функції, які використовують бібліотеку **Pandas** для обробки даних. Pandas UDF дозволяють застосовувати **функції на рівні Python** з використанням можливостей векторизації Pandas і при цьому підтримують **розподілені обчислення** у Spark.\n",
        "\n",
        "**UDF (User Defined Function)** — це користувацька функція, яку можна застосувати до даних в Spark для виконання специфічних операцій. Стандартні UDF у PySpark використовують звичайні Python-функції, що може бути повільним для великих даних, оскільки обробка йде рядок за рядком.\n",
        "  \n",
        "**Pandas UDF** (також називають **Vectorized UDF**) працюють на рівні **векторів** (або серій Pandas), що дозволяє Spark працювати з ними значно швидше, завдяки векторизації та оптимізації через **Apache Arrow**.\n",
        "\n",
        "### Основні переваги **Pandas UDF**:\n",
        "- **Швидкість**: Використання векторизації та інтеграція з **Apache Arrow** дозволяє суттєво прискорити обробку даних, порівняно зі звичайними UDF.\n",
        "- **Простота**: Pandas UDF використовують знайомі інтерфейси Pandas, що робить їх простими у використанні для тих, хто знайомий із Pandas.\n",
        "- **Масштабованість**: Хоча Pandas зазвичай використовується для обробки даних у пам'яті, використання Pandas UDF дозволяє масштабувати ці операції для обробки великих обсягів даних у кластері.\n",
        "\n",
        "### Типи Pandas UDF:\n",
        "\n",
        "1. **Scalar UDF**:\n",
        "   - Використовується для обробки поелементно (аналог звичайного UDF).\n",
        "   - Функція отримує та повертає серію Pandas (колонку).\n",
        "   \n",
        "2. **Grouped Map UDF**:\n",
        "   - Використовується для обробки груп даних.\n",
        "   - Функція отримує і повертає **DataFrame Pandas** для кожної групи.\n",
        "   \n",
        "3. **Grouped Aggregate UDF**:\n",
        "   - Використовується для обробки та агрегації даних у групах.\n",
        "   - Функція повертає одне значення для кожної групи.\n",
        "   \n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqfSCYTPSMv1"
      },
      "source": [
        "> `pandas_udf` і `PandasUDFType` — це ключові компоненти в **PySpark**, що дозволяють створювати і використовувати **Pandas UDF** (векторизовані користувацькі функції), які значно прискорюють обробку даних шляхом векторизації та використання **Apache Arrow**.\n",
        "\n",
        "### 1. **`pandas_udf`**\n",
        "**`pandas_udf`** — це декоратор, який використовується для визначення **Pandas UDF**. За допомогою цього декоратора ви можете вказати тип функції і тип даних, які вона буде обробляти або повертати.\n",
        "\n",
        "*Синтаксис*\n",
        "```python\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "\n",
        "@pandas_udf(returnType)\n",
        "def my_udf(col: pd.Series) -> pd.Series:\n",
        "    return col * 2\n",
        "```\n",
        "- **`returnType`** — це тип даних, який повертає функція. Ви можете вказати тип, наприклад, `StringType`, `DoubleType`, або безпосередньо `\"double\"`, `\"int\"`.\n",
        "- Функція може приймати і повертати **Pandas Series**, що дозволяє виконувати операції векторно.\n",
        "\n",
        "### 2. **`PandasUDFType`**\n",
        "**`PandasUDFType`** — це перелік типів Pandas UDF, що визначають, як саме функція обробляє дані. У новіших версіях PySpark тип UDF можна просто передати як параметр до **`pandas_udf`**, але `PandasUDFType` може бути використано для уточнення типу.\n",
        "\n",
        "#### Основні типи `PandasUDFType`:\n",
        "1.  **`SCALAR` (поелементна обробка)**:\n",
        "   - Обробляє кожен елемент колонки (векторно).\n",
        "   - Вхід і вихід — це **Pandas Series**.\n",
        "2. **`GROUPED_MAP` (обробка груп рядків)**:\n",
        "   - Обробляє кожну групу як **Pandas DataFrame**, повертаючи DataFrame.\n",
        "   - Використовується з функціями типу **`groupBy().apply()`**.\n",
        "3. **`GROUPED_AGG` (агрегація груп)**:\n",
        "   - Використовується для виконання агрегацій (підрахунку, середнього, суми тощо) для групованих даних.\n",
        "   - Вхід — це **Pandas Series**, вихід — одне значення.\n",
        "\n",
        "*Синтаксис*\n",
        "```python\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
        "import pandas as pd\n",
        "\n",
        "# Визначення функції Grouped Map UDF\n",
        "@pandas_udf(\"name string, avg_age double\", PandasUDFType.GROUPED_MAP)\n",
        "def average_age(pdf: pd.DataFrame) -> pd.DataFrame:\n",
        "    return pdf.assign(avg_age=pdf[\"age\"].mean())\n",
        "\n",
        "# Використання цієї функції\n",
        "df.groupby(\"name\").apply(average_age).show()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5OkmM3aRX-0"
      },
      "outputs": [],
      "source": [
        "# імпорт функцій Pandas UDF\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHenUxSGRyok"
      },
      "outputs": [],
      "source": [
        "@pandas_udf(\"float\")\n",
        "def convert_wt(s: pd.Series) -> pd.Series:\n",
        "    # Формула для перерахунку в метричну систему (1 фунт ≈ 0.45 кг)\n",
        "    return s * 0.45\n",
        "\n",
        "spark.udf.register(\"convert_weight\", convert_wt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FA2QDxySVcNo"
      },
      "outputs": [],
      "source": [
        "# застосуємо створену функцію до таблиці cars\n",
        "spark.sql(\"SELECT *, wt AS weight_imperial, convert_weight(wt) as weight_metric FROM cars\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0vqDFdl7mZx"
      },
      "source": [
        "##<center>__Самостійні завдання__</center>\n",
        "\n",
        "> Скопіювати блок самостійних завдань в окремий файл ***LastName_CP4.ipynb***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OflG6K-YVWbw"
      },
      "source": [
        "### Завдання №1\n",
        "1. Інсталювати та імпортувати необхідні бібліотеки та модулі\n",
        "2. Створити Spark Session\n",
        "2. Завантажити та перетворити датасет `mtcars` з лекційної частини у DataFrame PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vu87JoBTOfH"
      },
      "outputs": [],
      "source": [
        "# МІСЦЕ ДЛЯ КОДУ\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LirTJrM7W-cg"
      },
      "source": [
        "### Завдання №2\n",
        "Використовуючи Spark SQL виконати наступні запити\n",
        "1. Обчислити середнє значення витрат палива (`mpg`) для кожного типу передач (`am`) (0 = автоматична, 1 = ручна передача)\n",
        "2. Знайдіть три автомобілі з найбільшим значенням потужності `hp`\n",
        "3. Підрахувати кількість автомобілів з різними кількостями циліндрів (`cyl`)\n",
        "4. Визначте середню вагу `wt` автомобілів з ручною та автоматичною передачею\n",
        "5. Виберіть автомобілі, у яких `hp` більше 150 та `mpg` більше 20.\n",
        "6. Створіть запит на свій розсуд\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R4pqDIyW96X"
      },
      "outputs": [],
      "source": [
        "# МІСЦЕ ДЛЯ КОДУ\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnzAtVsKZkjc"
      },
      "source": [
        "### Завдання №3\n",
        "\n",
        "Створіть користувацькі функції Pandas UDF (User Defined Functions):\n",
        "1. Виведіть квадрат значення потужності (`hp`) для кожного автомобіля та виведіть результат як новий стовпчик таблиці\n",
        "2. Створіть UDF, яка нормалізує вагу автомобілів `wt`, використовуючи мінімаксний принцип нормалізації та виведіть результат як новий стовпчик таблиці\n",
        "3. Створіть UDF, яка обчислює коефіцієнт потужність/вага для кожного автомобіля та виведіть результат як новий стовпчик таблиці\n",
        "4. Створіть UDF, яка присвоює автомобілю категорію (малий, середній або великий) на основі кількості циліндрів (`cyl`) та виведіть результат як новий стовпчик таблиці\n",
        "5. Створіть UDF, яка визначає, чи є автомобіль економічним (якщо `mpg > 25`) і приймає бінарне значення та виведіть результат як новий стовпчик таблиці\n",
        "6. Створіть UDF на свій розсуд та виведіть результат як новий стовпчик таблиці"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "royz-qkJZmQ8"
      },
      "outputs": [],
      "source": [
        "# МІСЦЕ ДЛЯ КОДУ\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
