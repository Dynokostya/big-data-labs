{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jLE0nxT_P19"
      },
      "source": [
        "\n",
        "<center><font size=\"6\"><b>Комп'ютерний практикум 4.\n",
        "\n",
        " Основи PySpark: обробка даних DataFrame</b></font></center>\n",
        "\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb_WlwqB8hrg"
      },
      "source": [
        "<center><img src=\"https://media.licdn.com/dms/image/v2/C4E12AQEb6oxAxtYD-Q/article-cover_image-shrink_600_2000/article-cover_image-shrink_600_2000/0/1620420835464?e=2147483647&v=beta&t=EWVtIY_UWl1XNucKkGMrn6ooEuwOBaQ313Z1TarbZgk\" width=\"400\"></center>\n",
        "\n",
        "\n",
        "**PySpark** — це інтерфейс для використання **Apache Spark** з мовою програмування **Python**.\n",
        "\n",
        "**Apache Spark** — це потужний фреймворк для розподіленої обробки пакетних даних у кластері. PySpark надає Python API для інтеграції цих можливостей.\n",
        "\n",
        "### Основні особливості PySpark:\n",
        "\n",
        "1. **Обробка великих даних**: PySpark дозволяє обробляти великі набори даних розподілено на кількох вузлах у кластері, використовуючи потужні обчислювальні кластери, що пришвидшує виконання операцій.\n",
        "   \n",
        "2. **API для Python**: PySpark надає простий API для роботи з даними, який включає операції над RDD (Resilient Distributed Datasets), DataFrames, SQL, та Machine Learning.\n",
        "\n",
        "3. **Інструменти для обробки даних у реальному часі**: PySpark підтримує обробку даних у потоковому режимі через модуль **Spark Streaming**.\n",
        "\n",
        "4. **Машинне навчання**: PySpark включає бібліотеку для машинного навчання (MLlib), що дозволяє виконувати завдання класифікації, кластеризації, регресії та зниження розмірності.\n",
        "\n",
        "### Використання:\n",
        "- PySpark активно використовується для ___ETL-процесів___, де необхідно обробляти великі обсяги даних.\n",
        "- Використовується для ___аналітики___ та ___машинного навчання___ на масштабних даних.\n",
        "- Є популярним інструментом в середовищах ___Big Data___, таких як Amazon EMR, Google Dataproc та Microsoft Azure HDInsight.\n",
        "\n",
        "PySpark робить можливим використання простоти Python для роботи з великими даними в Apache Spark, що забезпечує гнучкість і масштабованість обчислень.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDMPJL418PuJ"
      },
      "source": [
        "> __Spark SQL__ - це модуль Spark для обробки структурованих даних. Він призначений для запитів до структурованих даних у програмах Spark, використовуючи або SQL, або знайомий API DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h2YOKaRS9BYm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in ./.venv/lib/python3.10/site-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in ./.venv/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: findspark in ./.venv/lib/python3.10/site-packages (2.0.1)\n",
            "Requirement already satisfied: pyarrow in ./.venv/lib/python3.10/site-packages (17.0.0)\n",
            "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.16.6 in ./.venv/lib/python3.10/site-packages (from pyarrow) (2.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Встановлюємо необхідні пакети\n",
        "! pip install pyspark\n",
        "! pip install findspark\n",
        "! pip install pyarrow pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MUWZYDVj8eT0"
      },
      "outputs": [],
      "source": [
        "# імпорт допоміжної бібліотеки для організації правильного шляху до Spark\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYU6BmAc-eel"
      },
      "source": [
        "Для роботи з **Apache Spark** у Python через бібліотеку **PySpark** необхідно імпортувати наступні модулі\n",
        "\n",
        "**`SparkContext`**:\n",
        "   - Це об'єкт, який є точкою входу в функціональні можливості Spark. Він представляє з'єднання програми зі **Spark кластером** (або локальним екземпляром Spark).\n",
        "   - `SparkContext` використовується для створення **RDD (Resilient Distributed Dataset)**, а також для взаємодії зі Spark API. Він керує роботою і розподілом завдань між вузлами кластера.\n",
        "   - У сучасних версіях Spark, `SparkContext` зазвичай створюється автоматично всередині **`SparkSession`**, тому його часто не потрібно ініціалізувати окремо, якщо використовується `SparkSession`.\n",
        "\n",
        "**`SparkConf`**:\n",
        "   - `SparkConf` дозволяє налаштувати параметри роботи Spark-додатка, такі як ім'я додатка, кількість ядер, кількість пам'яті тощо. Об'єкт `SparkConf` при створенні передається `SparkContext` або `SparkSession`, щоб визначити, як додаток повинен працювати.\n",
        "   - Це спосіб налаштувати специфічні параметри роботи Spark на рівні додатку.\n",
        "\n",
        "**`SparkSession`**:\n",
        "   - Це вищий рівень абстракції, який був введений у Spark 2.0. `SparkSession` об'єднує всі попередні функціональні можливості **SparkContext**, **SQLContext** і **HiveContext** в одному об'єкті. Це єдина точка входу для роботи з різними компонентами Spark (RDD, DataFrame, SQL-запити).\n",
        "   - `SparkSession` також використовується для роботи з **DataFrame**, SQL-запитами та інтеграції з іншими компонентами, такими як **Spark SQL**, **Streaming**, **MLlib** та **GraphX**.\n",
        "> **Основні можливості `SparkSession`**:\n",
        "   - Робота з `DataFrame` та SQL-запитами.\n",
        "   - Використання `Spark SQL` для взаємодії з даними у стилі SQL.\n",
        "   - Створення і керування RDD.\n",
        "   - Інтеграція з різними джерелами даних (HDFS, Cassandra, S3 та ін.).\n",
        "\n",
        "### Коли використовувати?\n",
        "\n",
        "- **`SparkContext`**: Якщо ви працюєте зі **старими версіями Spark** або хочете безпосередньо взаємодіяти з RDD.\n",
        "- **`SparkSession`**: У більшості сучасних додатків, починаючи з **Spark 2.0**, рекомендовано використовувати **SparkSession** для створення та керування всіма обчислювальними компонентами (DataFrame, SQL-запити та RDD).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "s07FBcRA94Zq"
      },
      "outputs": [],
      "source": [
        "#імпорт бібліотек та модулів\n",
        "\n",
        "import pandas as pd\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97rJQSzw61Hk"
      },
      "source": [
        "## Створення Spark сесії\n",
        "\n",
        "- `.builder` - це спосіб налаштувати і створити `SparkSession`, який є головним об'єктом у сучасних версіях Spark\n",
        "- `.appName(\"name\")` задає ім'я програми Spark для ідентифікації додатку під час його роботи в кластері або в локальному середовищі\n",
        "- `.config()` -  метод дозволяє передавати додаткові конфігураційні параметри для Spark\n",
        "- `.getOrCreate()` - метод або створює новий об'єкт `SparkSession`, або повертає вже існуючий, якщо такий є."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tgYEg7Pl7g7A"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/10/26 11:07:36 WARN Utils: Your hostname, ubuntu-server resolves to a loopback address: 127.0.1.1; using 192.168.50.183 instead (on interface eno1)\n",
            "24/10/26 11:07:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "24/10/26 11:07:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "# Створення об'єкту spark context class (не обов'язково)\n",
        "sc = SparkContext()\n",
        "\n",
        "# Створення spark session\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"My_Spark_basic\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PwHgEeqZE_km"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://192.168.50.183:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fb20eb31a80>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Для роботи з датафреймами необхідно переконатися, що екземпляр сеансу spark створено.\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHZ9JQ5w-RKf"
      },
      "source": [
        "## Завантаження даних\n",
        "\n",
        "> Для завантаження даних спочатку імпортується CSV-файл у таблицю даних `Pandas`, а потім передається в таблицю даних `Spark`\n",
        "\n",
        "Для створення фрейму даних `Spark` завантажуємо зовнішній фрейм даних, який називається `mtcars`. Цей фрейм даних містить 32 спостереження та 11 змінних:\n",
        "\n",
        "| colIndex | colName | units/description |\n",
        "| :---: | :--- | :--- |\n",
        "|[, 1] | mpg |Miles per gallon  |\n",
        "|[, 2] | cyl | Number of cylinders  |\n",
        "|[, 3] | disp | Displacement (cu.in.) |  \n",
        "|[, 4] | hp  | Gross horsepower  |\n",
        "|[, 5] | drat | Rear axle ratio  |\n",
        "|[, 6] | wt | Weight (lb/1000)  |\n",
        "|[, 7] | qsec | 1/4 mile time  |\n",
        "|[, 8] | vs  | V/S  |\n",
        "|[, 9] | am | Transmission (0 = automatic, 1 = manual)  |\n",
        "|[,10] | gear | Number of forward gears  |\n",
        "|[,11] | carb | Number of carburetors |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8r6bYJSB-klj"
      },
      "outputs": [],
      "source": [
        "# Використаємо `read_csv` з pandas для завантаження датасету\n",
        "mtcars = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/mtcars.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pa0WnmD_GY8U"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>mpg</th>\n",
              "      <th>cyl</th>\n",
              "      <th>disp</th>\n",
              "      <th>hp</th>\n",
              "      <th>drat</th>\n",
              "      <th>wt</th>\n",
              "      <th>qsec</th>\n",
              "      <th>vs</th>\n",
              "      <th>am</th>\n",
              "      <th>gear</th>\n",
              "      <th>carb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mazda RX4</td>\n",
              "      <td>21.0</td>\n",
              "      <td>6</td>\n",
              "      <td>160.0</td>\n",
              "      <td>110</td>\n",
              "      <td>3.90</td>\n",
              "      <td>2.620</td>\n",
              "      <td>16.46</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mazda RX4 Wag</td>\n",
              "      <td>21.0</td>\n",
              "      <td>6</td>\n",
              "      <td>160.0</td>\n",
              "      <td>110</td>\n",
              "      <td>3.90</td>\n",
              "      <td>2.875</td>\n",
              "      <td>17.02</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Datsun 710</td>\n",
              "      <td>22.8</td>\n",
              "      <td>4</td>\n",
              "      <td>108.0</td>\n",
              "      <td>93</td>\n",
              "      <td>3.85</td>\n",
              "      <td>2.320</td>\n",
              "      <td>18.61</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hornet 4 Drive</td>\n",
              "      <td>21.4</td>\n",
              "      <td>6</td>\n",
              "      <td>258.0</td>\n",
              "      <td>110</td>\n",
              "      <td>3.08</td>\n",
              "      <td>3.215</td>\n",
              "      <td>19.44</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hornet Sportabout</td>\n",
              "      <td>18.7</td>\n",
              "      <td>8</td>\n",
              "      <td>360.0</td>\n",
              "      <td>175</td>\n",
              "      <td>3.15</td>\n",
              "      <td>3.440</td>\n",
              "      <td>17.02</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Unnamed: 0   mpg  cyl   disp   hp  drat     wt   qsec  vs  am  gear  \\\n",
              "0          Mazda RX4  21.0    6  160.0  110  3.90  2.620  16.46   0   1     4   \n",
              "1      Mazda RX4 Wag  21.0    6  160.0  110  3.90  2.875  17.02   0   1     4   \n",
              "2         Datsun 710  22.8    4  108.0   93  3.85  2.320  18.61   1   1     4   \n",
              "3     Hornet 4 Drive  21.4    6  258.0  110  3.08  3.215  19.44   1   0     3   \n",
              "4  Hornet Sportabout  18.7    8  360.0  175  3.15  3.440  17.02   0   0     3   \n",
              "\n",
              "   carb  \n",
              "0     4  \n",
              "1     4  \n",
              "2     1  \n",
              "3     1  \n",
              "4     2  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Перегляд датасету\n",
        "mtcars.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "E2qOe-SuGkEF"
      },
      "outputs": [],
      "source": [
        "# переіменуємо перший стовпчик\n",
        "mtcars.rename( columns={'Unnamed: 0':'name'}, inplace=True )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIzFPdPLG3OY"
      },
      "source": [
        "> Функція `createDataFrame` завантажує дані в spark dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Vz7DY4sAHK8j"
      },
      "outputs": [],
      "source": [
        "sdf = spark.createDataFrame(mtcars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-A3cMZcHhIk"
      },
      "source": [
        "> Метод **`printSchema()`** використовується для виведення **схеми** (структури) **DataFrame** у Spark.\n",
        "\n",
        "**Схема** — це структура **DataFrame**, яка визначає назви стовпців і типи даних у цих стовпцях (наприклад, `StringType`, `IntegerType`, `DoubleType` і т.д.). Ця структура дозволяє знати, які дані містить DataFrame та в якому форматі вони зберігаються.\n",
        "\n",
        "Використовується щоб перевірити структуру даних у DataFrame перед виконанням операцій та для верифікації того, що типи даних і назви стовпців вірні, особливо якщо дані із зовнішніх джерел (файли CSV, бази даних тощо)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XvB5n6RxHVEg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- mpg: double (nullable = true)\n",
            " |-- cyl: long (nullable = true)\n",
            " |-- disp: double (nullable = true)\n",
            " |-- hp: long (nullable = true)\n",
            " |-- drat: double (nullable = true)\n",
            " |-- wt: double (nullable = true)\n",
            " |-- qsec: double (nullable = true)\n",
            " |-- vs: long (nullable = true)\n",
            " |-- am: long (nullable = true)\n",
            " |-- gear: long (nullable = true)\n",
            " |-- carb: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sdf.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdvmZoL5Ik_F"
      },
      "source": [
        "> Функція `withColumnRenamed()` перейменовує існуючі назви стовпців.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fF3Qa3loIqz9"
      },
      "outputs": [],
      "source": [
        "#Переіменовуємо назву стовпця «vs» на «versus» і зберігаємо зміни в новий DataFrame  «sdf_new».\n",
        "sdf_new = sdf.withColumnRenamed(\"vs\", \"versus\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tkgb7ErcJGH6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Row(name='Mazda RX4', mpg=21.0, cyl=6, disp=160.0, hp=110, drat=3.9, wt=2.62, qsec=16.46, versus=0, am=1, gear=4, carb=4)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sdf_new.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot3d7m4ZJUhc"
      },
      "source": [
        "## Створення тимчасової таблиці (Table View/табличне представлення)\n",
        "Створення табличного представлення в Spark SQL необхідне для програмного запуску SQL запитів для DataFrame. Представлення - це тимчасова таблиця для виконання SQL запитів, яке забезпечує локальну область видимості в межах поточного сеансу Spark за допомогою функції `createTempView()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uIz1UJ52Jvqt"
      },
      "outputs": [],
      "source": [
        "sdf.createTempView(\"cars\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "657E_NTHM0yL"
      },
      "source": [
        "У `PySpark` для роботи з `DataFrame` часто використовують методи, схожі на SQL-запити.\n",
        "\n",
        "__<center>Таблиця основних SQL-запитів та їх еквівалентів у PySpark:</center>__\n",
        "\n",
        "| **SQL Запит**               | **PySpark (DataFrame API)**                                              | **Опис**                                      |\n",
        "|-----------------------------|---------------------------------------------------------------------------|-----------------------------------------------|\n",
        "| **SELECT**                   | `df.select(\"column1\", \"column2\")`                                         | Вибір конкретних колонок                     |\n",
        "| **WHERE**                    | `df.filter(df[\"column\"] == value)`                                        | Фільтрація рядків за умовою                   |\n",
        "| **AND / OR**                 | `df.filter((df[\"col1\"] == val1) & (df[\"col2\"] == val2))`                  | Використання логічних операторів              |\n",
        "| **ORDER BY**                 | `df.orderBy(\"column\", ascending=False)`                                   | Сортування за колонкою                       |\n",
        "| **GROUP BY**                 | `df.groupBy(\"column\").agg({\"column\": \"sum\"})`                             | Групування даних і агрегація                  |\n",
        "| **HAVING**                   | `df.groupBy(\"column\").agg({\"column\": \"sum\"}).filter(\"sum(column) > value\")` | Фільтрація результатів після групування       |\n",
        "| **JOIN**                     | `df1.join(df2, df1[\"id\"] == df2[\"id\"], \"inner\")`                          | Об'єднання двох DataFrame                     |\n",
        "| **LIMIT**                    | `df.limit(10)`                                                           | Обмеження кількості рядків                   |\n",
        "| **DISTINCT**                 | `df.select(\"column\").distinct()`                                          | Вибір унікальних значень                     |\n",
        "| **COUNT**                    | `df.count()`                                                             | Підрахунок кількості рядків                  |\n",
        "| **SUM**                      | `df.groupBy().sum(\"column\")`                                              | Підсумовування значень у колонці             |\n",
        "| **AVG (Середнє значення)**   | `df.groupBy().avg(\"column\")`                                              | Обчислення середнього значення               |\n",
        "| **MAX (Максимум)**           | `df.groupBy().max(\"column\")`                                              | Визначення максимального значення            |\n",
        "| **MIN (Мінімум)**            | `df.groupBy().min(\"column\")`                                              | Визначення мінімального значення             |\n",
        "| **WITH ALIAS (Псевдоніми)**  | `df.select(df[\"column\"].alias(\"new_name\"))`                               | Присвоєння псевдонімів для колонок           |\n",
        "| **INNER JOIN**               | `df1.join(df2, df1[\"id\"] == df2[\"id\"], \"inner\")`                          | Внутрішнє об'єднання                         |\n",
        "| **LEFT JOIN**                | `df1.join(df2, df1[\"id\"] == df2[\"id\"], \"left\")`                           | Ліве об'єднання                              |\n",
        "| **RIGHT JOIN**               | `df1.join(df2, df1[\"id\"] == df2[\"id\"], \"right\")`                          | Праве об'єднання                             |\n",
        "| **FULL OUTER JOIN**          | `df1.join(df2, df1[\"id\"] == df2[\"id\"], \"outer\")`                          | Повне зовнішнє об'єднання                    |\n",
        "| **UNION**                    | `df1.union(df2)`                                                         | Об'єднання двох DataFrame (без дублікатів)   |\n",
        "| **UNION ALL**                | `df1.unionAll(df2)`                                                      | Об'єднання двох DataFrame (з дублікатами)    |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOv5NibCXaso"
      },
      "source": [
        "> `spark.sql` — це метод у **PySpark**, який дозволяє виконувати **SQL-запити** безпосередньо на **DataFrame**. Він використовується для інтеграції SQL з PySpark, дозволяючи працювати з великими даними у знайомому форматі SQL-запитів.\n",
        "\n",
        "1. **Створення тимчасової таблиці**: Це дозволяє звертатися до цього DataFrame в SQL-запитах, як до таблиці в базі даних.\n",
        "   \n",
        "2. **Виконання SQL-запитів**: Після створення тимчасової таблиці можна виконувати SQL-запити з використанням методу `spark.sql()`. Цей метод повертає результат як новий DataFrame.\n",
        "\n",
        "*Синтаксис*\n",
        "```python\n",
        "spark.sql(\"SQL-запит\")\n",
        "```\n",
        "### Переваги використання `spark.sql`:\n",
        "1. **Знайомий синтаксис**: SQL-запити.\n",
        "2. **Масштабованість**: SQL-запити, виконані через `spark.sql`, можуть працювати з великими наборами даних, розподіленими в кластері.\n",
        "3. **Гнучкість**: Можна комбінувати SQL із методами DataFrame API для виконання складних обчислень.\n",
        "\n",
        "`spark.sql` — це потужний інструмент у PySpark, який дозволяє використовувати SQL-запити для обробки великих даних. Він надає гнучкість у використанні стандартного SQL-синтаксису для вибірок, агрегацій та інших операцій на великих наборах даних, що зберігаються в Spark DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sHxOBl9RKfxg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
            "|               name| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|\n",
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
            "|          Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|\n",
            "|      Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|\n",
            "|         Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|\n",
            "|     Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|\n",
            "|  Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|\n",
            "|            Valiant|18.1|  6|225.0|105|2.76| 3.46|20.22|  1|  0|   3|   1|\n",
            "|         Duster 360|14.3|  8|360.0|245|3.21| 3.57|15.84|  0|  0|   3|   4|\n",
            "|          Merc 240D|24.4|  4|146.7| 62|3.69| 3.19| 20.0|  1|  0|   4|   2|\n",
            "|           Merc 230|22.8|  4|140.8| 95|3.92| 3.15| 22.9|  1|  0|   4|   2|\n",
            "|           Merc 280|19.2|  6|167.6|123|3.92| 3.44| 18.3|  1|  0|   4|   4|\n",
            "|          Merc 280C|17.8|  6|167.6|123|3.92| 3.44| 18.9|  1|  0|   4|   4|\n",
            "|         Merc 450SE|16.4|  8|275.8|180|3.07| 4.07| 17.4|  0|  0|   3|   3|\n",
            "|         Merc 450SL|17.3|  8|275.8|180|3.07| 3.73| 17.6|  0|  0|   3|   3|\n",
            "|        Merc 450SLC|15.2|  8|275.8|180|3.07| 3.78| 18.0|  0|  0|   3|   3|\n",
            "| Cadillac Fleetwood|10.4|  8|472.0|205|2.93| 5.25|17.98|  0|  0|   3|   4|\n",
            "|Lincoln Continental|10.4|  8|460.0|215| 3.0|5.424|17.82|  0|  0|   3|   4|\n",
            "|  Chrysler Imperial|14.7|  8|440.0|230|3.23|5.345|17.42|  0|  0|   3|   4|\n",
            "|           Fiat 128|32.4|  4| 78.7| 66|4.08|  2.2|19.47|  1|  1|   4|   1|\n",
            "|        Honda Civic|30.4|  4| 75.7| 52|4.93|1.615|18.52|  1|  1|   4|   2|\n",
            "|     Toyota Corolla|33.9|  4| 71.1| 65|4.22|1.835| 19.9|  1|  1|   4|   1|\n",
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Вивід всієї таблиці даних\n",
        "spark.sql(\"SELECT * FROM cars\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OBdMUJ3WKiT5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+\n",
            "| mpg|\n",
            "+----+\n",
            "|21.0|\n",
            "|21.0|\n",
            "|22.8|\n",
            "|21.4|\n",
            "|18.7|\n",
            "+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Вибір змінної mpg\n",
        "spark.sql(\"SELECT mpg FROM cars\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "L-k-H4pYKkyP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
            "|       name| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|\n",
            "+-----------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
            "| Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|\n",
            "|  Merc 240D|24.4|  4|146.7| 62|3.69| 3.19| 20.0|  1|  0|   4|   2|\n",
            "|   Merc 230|22.8|  4|140.8| 95|3.92| 3.15| 22.9|  1|  0|   4|   2|\n",
            "|   Fiat 128|32.4|  4| 78.7| 66|4.08|  2.2|19.47|  1|  1|   4|   1|\n",
            "|Honda Civic|30.4|  4| 75.7| 52|4.93|1.615|18.52|  1|  1|   4|   2|\n",
            "+-----------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Запит для визначення автомобілів з великим пробігом і малою кількістю циліндрів\n",
        "spark.sql(\"SELECT * FROM cars where mpg > 15 AND cyl < 6\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RGXSsbhXKnNs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
            "|               name| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|\n",
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
            "|         Duster 360|14.3|  8|360.0|245|3.21| 3.57|15.84|  0|  0|   3|   4|\n",
            "| Cadillac Fleetwood|10.4|  8|472.0|205|2.93| 5.25|17.98|  0|  0|   3|   4|\n",
            "|Lincoln Continental|10.4|  8|460.0|215| 3.0|5.424|17.82|  0|  0|   3|   4|\n",
            "|  Chrysler Imperial|14.7|  8|440.0|230|3.23|5.345|17.42|  0|  0|   3|   4|\n",
            "|         Camaro Z28|13.3|  8|350.0|245|3.73| 3.84|15.41|  0|  0|   3|   4|\n",
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Авто, які мають витрати палива менше 15 миль на галон\n",
        "sdf.where(sdf['mpg'] < 15).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "24fAK9HhKpdn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+---+\n",
            "|count(1)|cyl|\n",
            "+--------+---+\n",
            "|       7|  6|\n",
            "|      11|  4|\n",
            "|      14|  8|\n",
            "+--------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Агрегація даних та групування за циліндрами\n",
        "spark.sql(\"SELECT count(*), cyl from cars GROUP BY cyl\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLiC2TVkPc7b"
      },
      "source": [
        "## Створення Pandas UDF\n",
        "\n",
        "**Pandas UDF (User Defined Functions)** в **PySpark** — це користувацькі функції, які використовують бібліотеку **Pandas** для обробки даних. Pandas UDF дозволяють застосовувати **функції на рівні Python** з використанням можливостей векторизації Pandas і при цьому підтримують **розподілені обчислення** у Spark.\n",
        "\n",
        "**UDF (User Defined Function)** — це користувацька функція, яку можна застосувати до даних в Spark для виконання специфічних операцій. Стандартні UDF у PySpark використовують звичайні Python-функції, що може бути повільним для великих даних, оскільки обробка йде рядок за рядком.\n",
        "  \n",
        "**Pandas UDF** (також називають **Vectorized UDF**) працюють на рівні **векторів** (або серій Pandas), що дозволяє Spark працювати з ними значно швидше, завдяки векторизації та оптимізації через **Apache Arrow**.\n",
        "\n",
        "### Основні переваги **Pandas UDF**:\n",
        "- **Швидкість**: Використання векторизації та інтеграція з **Apache Arrow** дозволяє суттєво прискорити обробку даних, порівняно зі звичайними UDF.\n",
        "- **Простота**: Pandas UDF використовують знайомі інтерфейси Pandas, що робить їх простими у використанні для тих, хто знайомий із Pandas.\n",
        "- **Масштабованість**: Хоча Pandas зазвичай використовується для обробки даних у пам'яті, використання Pandas UDF дозволяє масштабувати ці операції для обробки великих обсягів даних у кластері.\n",
        "\n",
        "### Типи Pandas UDF:\n",
        "\n",
        "1. **Scalar UDF**:\n",
        "   - Використовується для обробки поелементно (аналог звичайного UDF).\n",
        "   - Функція отримує та повертає серію Pandas (колонку).\n",
        "   \n",
        "2. **Grouped Map UDF**:\n",
        "   - Використовується для обробки груп даних.\n",
        "   - Функція отримує і повертає **DataFrame Pandas** для кожної групи.\n",
        "   \n",
        "3. **Grouped Aggregate UDF**:\n",
        "   - Використовується для обробки та агрегації даних у групах.\n",
        "   - Функція повертає одне значення для кожної групи.\n",
        "   \n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqfSCYTPSMv1"
      },
      "source": [
        "> `pandas_udf` і `PandasUDFType` — це ключові компоненти в **PySpark**, що дозволяють створювати і використовувати **Pandas UDF** (векторизовані користувацькі функції), які значно прискорюють обробку даних шляхом векторизації та використання **Apache Arrow**.\n",
        "\n",
        "### 1. **`pandas_udf`**\n",
        "**`pandas_udf`** — це декоратор, який використовується для визначення **Pandas UDF**. За допомогою цього декоратора ви можете вказати тип функції і тип даних, які вона буде обробляти або повертати.\n",
        "\n",
        "*Синтаксис*\n",
        "```python\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "\n",
        "@pandas_udf(returnType)\n",
        "def my_udf(col: pd.Series) -> pd.Series:\n",
        "    return col * 2\n",
        "```\n",
        "- **`returnType`** — це тип даних, який повертає функція. Ви можете вказати тип, наприклад, `StringType`, `DoubleType`, або безпосередньо `\"double\"`, `\"int\"`.\n",
        "- Функція може приймати і повертати **Pandas Series**, що дозволяє виконувати операції векторно.\n",
        "\n",
        "### 2. **`PandasUDFType`**\n",
        "**`PandasUDFType`** — це перелік типів Pandas UDF, що визначають, як саме функція обробляє дані. У новіших версіях PySpark тип UDF можна просто передати як параметр до **`pandas_udf`**, але `PandasUDFType` може бути використано для уточнення типу.\n",
        "\n",
        "#### Основні типи `PandasUDFType`:\n",
        "1.  **`SCALAR` (поелементна обробка)**:\n",
        "   - Обробляє кожен елемент колонки (векторно).\n",
        "   - Вхід і вихід — це **Pandas Series**.\n",
        "2. **`GROUPED_MAP` (обробка груп рядків)**:\n",
        "   - Обробляє кожну групу як **Pandas DataFrame**, повертаючи DataFrame.\n",
        "   - Використовується з функціями типу **`groupBy().apply()`**.\n",
        "3. **`GROUPED_AGG` (агрегація груп)**:\n",
        "   - Використовується для виконання агрегацій (підрахунку, середнього, суми тощо) для групованих даних.\n",
        "   - Вхід — це **Pandas Series**, вихід — одне значення.\n",
        "\n",
        "*Синтаксис*\n",
        "```python\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
        "import pandas as pd\n",
        "\n",
        "# Визначення функції Grouped Map UDF\n",
        "@pandas_udf(\"name string, avg_age double\", PandasUDFType.GROUPED_MAP)\n",
        "def average_age(pdf: pd.DataFrame) -> pd.DataFrame:\n",
        "    return pdf.assign(avg_age=pdf[\"age\"].mean())\n",
        "\n",
        "# Використання цієї функції\n",
        "df.groupby(\"name\").apply(average_age).show()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "I5OkmM3aRX-0"
      },
      "outputs": [],
      "source": [
        "# імпорт функцій Pandas UDF\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZHenUxSGRyok"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<pyspark.sql.udf.UserDefinedFunction at 0x7fb243f10760>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "@pandas_udf(\"float\")\n",
        "def convert_wt(s: pd.Series) -> pd.Series:\n",
        "    # Формула для перерахунку в метричну систему (1 фунт ≈ 0.45 кг)\n",
        "    return s * 0.45\n",
        "\n",
        "spark.udf.register(\"convert_weight\", convert_wt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "FA2QDxySVcNo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+---------------+-------------+\n",
            "|               name| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|weight_imperial|weight_metric|\n",
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+---------------+-------------+\n",
            "|          Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|           2.62|        1.179|\n",
            "|      Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|          2.875|      1.29375|\n",
            "|         Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|           2.32|        1.044|\n",
            "|     Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|          3.215|      1.44675|\n",
            "|  Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|           3.44|        1.548|\n",
            "|            Valiant|18.1|  6|225.0|105|2.76| 3.46|20.22|  1|  0|   3|   1|           3.46|        1.557|\n",
            "|         Duster 360|14.3|  8|360.0|245|3.21| 3.57|15.84|  0|  0|   3|   4|           3.57|       1.6065|\n",
            "|          Merc 240D|24.4|  4|146.7| 62|3.69| 3.19| 20.0|  1|  0|   4|   2|           3.19|       1.4355|\n",
            "|           Merc 230|22.8|  4|140.8| 95|3.92| 3.15| 22.9|  1|  0|   4|   2|           3.15|       1.4175|\n",
            "|           Merc 280|19.2|  6|167.6|123|3.92| 3.44| 18.3|  1|  0|   4|   4|           3.44|        1.548|\n",
            "|          Merc 280C|17.8|  6|167.6|123|3.92| 3.44| 18.9|  1|  0|   4|   4|           3.44|        1.548|\n",
            "|         Merc 450SE|16.4|  8|275.8|180|3.07| 4.07| 17.4|  0|  0|   3|   3|           4.07|       1.8315|\n",
            "|         Merc 450SL|17.3|  8|275.8|180|3.07| 3.73| 17.6|  0|  0|   3|   3|           3.73|       1.6785|\n",
            "|        Merc 450SLC|15.2|  8|275.8|180|3.07| 3.78| 18.0|  0|  0|   3|   3|           3.78|        1.701|\n",
            "| Cadillac Fleetwood|10.4|  8|472.0|205|2.93| 5.25|17.98|  0|  0|   3|   4|           5.25|       2.3625|\n",
            "|Lincoln Continental|10.4|  8|460.0|215| 3.0|5.424|17.82|  0|  0|   3|   4|          5.424|       2.4408|\n",
            "|  Chrysler Imperial|14.7|  8|440.0|230|3.23|5.345|17.42|  0|  0|   3|   4|          5.345|      2.40525|\n",
            "|           Fiat 128|32.4|  4| 78.7| 66|4.08|  2.2|19.47|  1|  1|   4|   1|            2.2|         0.99|\n",
            "|        Honda Civic|30.4|  4| 75.7| 52|4.93|1.615|18.52|  1|  1|   4|   2|          1.615|      0.72675|\n",
            "|     Toyota Corolla|33.9|  4| 71.1| 65|4.22|1.835| 19.9|  1|  1|   4|   1|          1.835|      0.82575|\n",
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+---------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# застосуємо створену функцію до таблиці cars\n",
        "spark.sql(\"SELECT *, wt AS weight_imperial, convert_weight(wt) as weight_metric FROM cars\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0vqDFdl7mZx"
      },
      "source": [
        "##<center>__Самостійні завдання__</center>\n",
        "\n",
        "> Скопіювати блок самостійних завдань в окремий файл ***LastName_CP4.ipynb***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OflG6K-YVWbw"
      },
      "source": [
        "### Завдання №1\n",
        "1. Інсталювати та імпортувати необхідні бібліотеки та модулі\n",
        "2. Створити Spark Session\n",
        "2. Завантажити та перетворити датасет `mtcars` з лекційної частини у DataFrame PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vu87JoBTOfH"
      },
      "outputs": [],
      "source": [
        "# МІСЦЕ ДЛЯ КОДУ\n",
        "\n",
        "# Виконано в лекційній частині"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LirTJrM7W-cg"
      },
      "source": [
        "### Завдання №2\n",
        "Використовуючи Spark SQL виконати наступні запити\n",
        "1. Обчислити середнє значення витрат палива (`mpg`) для кожного типу передач (`am`) (0 = автоматична, 1 = ручна передача)\n",
        "2. Знайдіть три автомобілі з найбільшим значенням потужності `hp`\n",
        "3. Підрахувати кількість автомобілів з різними кількостями циліндрів (`cyl`)\n",
        "4. Визначте середню вагу `wt` автомобілів з ручною та автоматичною передачею\n",
        "5. Виберіть автомобілі, у яких `hp` більше 150 та `mpg` більше 20.\n",
        "6. Створіть запит на свій розсуд\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8R4pqDIyW96X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------------------+\n",
            "| am|           avg_mpg|\n",
            "+---+------------------+\n",
            "|  1|24.392307692307693|\n",
            "|  0|17.147368421052633|\n",
            "+---+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# МІСЦЕ ДЛЯ КОДУ\n",
        "# 1\n",
        "spark.sql(\"SELECT am, AVG(mpg) AS avg_mpg FROM cars GROUP BY am\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+---+\n",
            "|          name| hp|\n",
            "+--------------+---+\n",
            "| Maserati Bora|335|\n",
            "|Ford Pantera L|264|\n",
            "|    Camaro Z28|245|\n",
            "+--------------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 2\n",
        "spark.sql(\"SELECT name, hp FROM cars ORDER BY hp DESC LIMIT 3\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---------+\n",
            "|cyl|car_count|\n",
            "+---+---------+\n",
            "|  6|        7|\n",
            "|  4|       11|\n",
            "|  8|       14|\n",
            "+---+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 3\n",
        "spark.sql(\"SELECT cyl, COUNT(*) AS car_count FROM cars GROUP BY cyl\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------------------+\n",
            "| am|        avg_weight|\n",
            "+---+------------------+\n",
            "|  1|2.4110000000000005|\n",
            "|  0| 3.768894736842105|\n",
            "+---+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 4\n",
        "spark.sql(\"SELECT am, AVG(wt) AS avg_weight FROM cars GROUP BY am\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+---+---+----+---+----+---+----+---+---+----+----+\n",
            "|name|mpg|cyl|disp| hp|drat| wt|qsec| vs| am|gear|carb|\n",
            "+----+---+---+----+---+----+---+----+---+---+----+----+\n",
            "+----+---+---+----+---+----+---+----+---+---+----+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 5\n",
        "spark.sql(\"SELECT * FROM cars WHERE hp > 150 AND mpg > 20\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+----+---+-----+\n",
            "|          name| mpg| hp|   wt|\n",
            "+--------------+----+---+-----+\n",
            "|Toyota Corolla|33.9| 65|1.835|\n",
            "|      Fiat 128|32.4| 66|  2.2|\n",
            "|  Lotus Europa|30.4|113|1.513|\n",
            "|   Honda Civic|30.4| 52|1.615|\n",
            "|     Fiat X1-9|27.3| 66|1.935|\n",
            "| Porsche 914-2|26.0| 91| 2.14|\n",
            "+--------------+----+---+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 6\n",
        "spark.sql(\"SELECT name, mpg, hp, wt FROM cars WHERE wt < 3 AND mpg > 25 ORDER BY mpg DESC\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnzAtVsKZkjc"
      },
      "source": [
        "### Завдання №3\n",
        "\n",
        "Створіть користувацькі функції Pandas UDF (User Defined Functions):\n",
        "1. Виведіть квадрат значення потужності (`hp`) для кожного автомобіля та виведіть результат як новий стовпчик таблиці\n",
        "2. Створіть UDF, яка нормалізує вагу автомобілів `wt`, використовуючи мінімаксний принцип нормалізації та виведіть результат як новий стовпчик таблиці\n",
        "3. Створіть UDF, яка обчислює коефіцієнт потужність/вага для кожного автомобіля та виведіть результат як новий стовпчик таблиці\n",
        "4. Створіть UDF, яка присвоює автомобілю категорію (малий, середній або великий) на основі кількості циліндрів (`cyl`) та виведіть результат як новий стовпчик таблиці\n",
        "5. Створіть UDF, яка визначає, чи є автомобіль економічним (якщо `mpg > 25`) і приймає бінарне значення та виведіть результат як новий стовпчик таблиці\n",
        "6. Створіть UDF на свій розсуд та виведіть результат як новий стовпчик таблиці"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "royz-qkJZmQ8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+----------+\n",
            "|               name| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|hp_squared|\n",
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+----------+\n",
            "|          Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|   12100.0|\n",
            "|      Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|   12100.0|\n",
            "|         Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|    8649.0|\n",
            "|     Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|   12100.0|\n",
            "|  Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|   30625.0|\n",
            "|            Valiant|18.1|  6|225.0|105|2.76| 3.46|20.22|  1|  0|   3|   1|   11025.0|\n",
            "|         Duster 360|14.3|  8|360.0|245|3.21| 3.57|15.84|  0|  0|   3|   4|   60025.0|\n",
            "|          Merc 240D|24.4|  4|146.7| 62|3.69| 3.19| 20.0|  1|  0|   4|   2|    3844.0|\n",
            "|           Merc 230|22.8|  4|140.8| 95|3.92| 3.15| 22.9|  1|  0|   4|   2|    9025.0|\n",
            "|           Merc 280|19.2|  6|167.6|123|3.92| 3.44| 18.3|  1|  0|   4|   4|   15129.0|\n",
            "|          Merc 280C|17.8|  6|167.6|123|3.92| 3.44| 18.9|  1|  0|   4|   4|   15129.0|\n",
            "|         Merc 450SE|16.4|  8|275.8|180|3.07| 4.07| 17.4|  0|  0|   3|   3|   32400.0|\n",
            "|         Merc 450SL|17.3|  8|275.8|180|3.07| 3.73| 17.6|  0|  0|   3|   3|   32400.0|\n",
            "|        Merc 450SLC|15.2|  8|275.8|180|3.07| 3.78| 18.0|  0|  0|   3|   3|   32400.0|\n",
            "| Cadillac Fleetwood|10.4|  8|472.0|205|2.93| 5.25|17.98|  0|  0|   3|   4|   42025.0|\n",
            "|Lincoln Continental|10.4|  8|460.0|215| 3.0|5.424|17.82|  0|  0|   3|   4|   46225.0|\n",
            "|  Chrysler Imperial|14.7|  8|440.0|230|3.23|5.345|17.42|  0|  0|   3|   4|   52900.0|\n",
            "|           Fiat 128|32.4|  4| 78.7| 66|4.08|  2.2|19.47|  1|  1|   4|   1|    4356.0|\n",
            "|        Honda Civic|30.4|  4| 75.7| 52|4.93|1.615|18.52|  1|  1|   4|   2|    2704.0|\n",
            "|     Toyota Corolla|33.9|  4| 71.1| 65|4.22|1.835| 19.9|  1|  1|   4|   1|    4225.0|\n",
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# МІСЦЕ ДЛЯ КОДУ\n",
        "\n",
        "from pyspark.sql.functions import col, pow, udf\n",
        "from pyspark.sql.types import DoubleType, StringType, IntegerType\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# 1\n",
        "@pandas_udf(\"float\")\n",
        "def square_hp(s: pd.Series) -> pd.Series:\n",
        "    return s ** 2\n",
        "\n",
        "spark.udf.register(\"square_hp\", square_hp)\n",
        "spark.sql(\"SELECT *, square_hp(hp) AS hp_squared FROM cars\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+-----------------+\n",
            "|               name| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|normalized_weight|\n",
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+-----------------+\n",
            "|          Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|       0.28304783|\n",
            "|      Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|       0.34824854|\n",
            "|         Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|       0.20634109|\n",
            "|     Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|        0.4351828|\n",
            "|  Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|       0.49271286|\n",
            "|            Valiant|18.1|  6|225.0|105|2.76| 3.46|20.22|  1|  0|   3|   1|       0.49782664|\n",
            "|         Duster 360|14.3|  8|360.0|245|3.21| 3.57|15.84|  0|  0|   3|   4|       0.52595246|\n",
            "|          Merc 240D|24.4|  4|146.7| 62|3.69| 3.19| 20.0|  1|  0|   4|   2|        0.4287906|\n",
            "|           Merc 230|22.8|  4|140.8| 95|3.92| 3.15| 22.9|  1|  0|   4|   2|       0.41856304|\n",
            "|           Merc 280|19.2|  6|167.6|123|3.92| 3.44| 18.3|  1|  0|   4|   4|       0.49271286|\n",
            "|          Merc 280C|17.8|  6|167.6|123|3.92| 3.44| 18.9|  1|  0|   4|   4|       0.49271286|\n",
            "|         Merc 450SE|16.4|  8|275.8|180|3.07| 4.07| 17.4|  0|  0|   3|   3|         0.653797|\n",
            "|         Merc 450SL|17.3|  8|275.8|180|3.07| 3.73| 17.6|  0|  0|   3|   3|        0.5668627|\n",
            "|        Merc 450SLC|15.2|  8|275.8|180|3.07| 3.78| 18.0|  0|  0|   3|   3|        0.5796471|\n",
            "| Cadillac Fleetwood|10.4|  8|472.0|205|2.93| 5.25|17.98|  0|  0|   3|   4|        0.9555101|\n",
            "|Lincoln Continental|10.4|  8|460.0|215| 3.0|5.424|17.82|  0|  0|   3|   4|              1.0|\n",
            "|  Chrysler Imperial|14.7|  8|440.0|230|3.23|5.345|17.42|  0|  0|   3|   4|        0.9798006|\n",
            "|           Fiat 128|32.4|  4| 78.7| 66|4.08|  2.2|19.47|  1|  1|   4|   1|        0.1756584|\n",
            "|        Honda Civic|30.4|  4| 75.7| 52|4.93|1.615|18.52|  1|  1|   4|   2|      0.026080286|\n",
            "|     Toyota Corolla|33.9|  4| 71.1| 65|4.22|1.835| 19.9|  1|  1|   4|   1|       0.08233188|\n",
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 2\n",
        "wt_min, wt_max = sdf.agg({\"wt\": \"min\"}).collect()[0][0], sdf.agg({\"wt\": \"max\"}).collect()[0][0]\n",
        "\n",
        "@pandas_udf(\"float\")\n",
        "def normalize_wt(s: pd.Series) -> pd.Series:\n",
        "    return (s - wt_min) / (wt_max - wt_min)\n",
        "\n",
        "spark.udf.register(\"normalize_weight\", normalize_wt)\n",
        "spark.sql(\"SELECT *, normalize_weight(wt) AS normalized_weight FROM cars\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+------------------+\n",
            "|               name| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|power_weight_ratio|\n",
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+------------------+\n",
            "|          Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|         41.984734|\n",
            "|      Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|         38.260868|\n",
            "|         Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|          40.08621|\n",
            "|     Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|          34.21462|\n",
            "|  Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|         50.872093|\n",
            "|            Valiant|18.1|  6|225.0|105|2.76| 3.46|20.22|  1|  0|   3|   1|          30.34682|\n",
            "|         Duster 360|14.3|  8|360.0|245|3.21| 3.57|15.84|  0|  0|   3|   4|          68.62745|\n",
            "|          Merc 240D|24.4|  4|146.7| 62|3.69| 3.19| 20.0|  1|  0|   4|   2|         19.435738|\n",
            "|           Merc 230|22.8|  4|140.8| 95|3.92| 3.15| 22.9|  1|  0|   4|   2|          30.15873|\n",
            "|           Merc 280|19.2|  6|167.6|123|3.92| 3.44| 18.3|  1|  0|   4|   4|         35.755814|\n",
            "|          Merc 280C|17.8|  6|167.6|123|3.92| 3.44| 18.9|  1|  0|   4|   4|         35.755814|\n",
            "|         Merc 450SE|16.4|  8|275.8|180|3.07| 4.07| 17.4|  0|  0|   3|   3|         44.226044|\n",
            "|         Merc 450SL|17.3|  8|275.8|180|3.07| 3.73| 17.6|  0|  0|   3|   3|         48.257374|\n",
            "|        Merc 450SLC|15.2|  8|275.8|180|3.07| 3.78| 18.0|  0|  0|   3|   3|          47.61905|\n",
            "| Cadillac Fleetwood|10.4|  8|472.0|205|2.93| 5.25|17.98|  0|  0|   3|   4|          39.04762|\n",
            "|Lincoln Continental|10.4|  8|460.0|215| 3.0|5.424|17.82|  0|  0|   3|   4|          39.63864|\n",
            "|  Chrysler Imperial|14.7|  8|440.0|230|3.23|5.345|17.42|  0|  0|   3|   4|          43.03087|\n",
            "|           Fiat 128|32.4|  4| 78.7| 66|4.08|  2.2|19.47|  1|  1|   4|   1|              30.0|\n",
            "|        Honda Civic|30.4|  4| 75.7| 52|4.93|1.615|18.52|  1|  1|   4|   2|         32.198143|\n",
            "|     Toyota Corolla|33.9|  4| 71.1| 65|4.22|1.835| 19.9|  1|  1|   4|   1|         35.422344|\n",
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 3\n",
        "@pandas_udf(\"float\")\n",
        "def power_to_weight(s_hp: pd.Series, s_wt: pd.Series) -> pd.Series:\n",
        "    return s_hp / s_wt\n",
        "\n",
        "spark.udf.register(\"power_to_weight_ratio\", power_to_weight)\n",
        "spark.sql(\"SELECT *, power_to_weight_ratio(hp, wt) AS power_weight_ratio FROM cars\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+------------+\n",
            "|               name| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|car_category|\n",
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+------------+\n",
            "|          Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|      medium|\n",
            "|      Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|      medium|\n",
            "|         Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|       small|\n",
            "|     Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|      medium|\n",
            "|  Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|       large|\n",
            "|            Valiant|18.1|  6|225.0|105|2.76| 3.46|20.22|  1|  0|   3|   1|      medium|\n",
            "|         Duster 360|14.3|  8|360.0|245|3.21| 3.57|15.84|  0|  0|   3|   4|       large|\n",
            "|          Merc 240D|24.4|  4|146.7| 62|3.69| 3.19| 20.0|  1|  0|   4|   2|       small|\n",
            "|           Merc 230|22.8|  4|140.8| 95|3.92| 3.15| 22.9|  1|  0|   4|   2|       small|\n",
            "|           Merc 280|19.2|  6|167.6|123|3.92| 3.44| 18.3|  1|  0|   4|   4|      medium|\n",
            "|          Merc 280C|17.8|  6|167.6|123|3.92| 3.44| 18.9|  1|  0|   4|   4|      medium|\n",
            "|         Merc 450SE|16.4|  8|275.8|180|3.07| 4.07| 17.4|  0|  0|   3|   3|       large|\n",
            "|         Merc 450SL|17.3|  8|275.8|180|3.07| 3.73| 17.6|  0|  0|   3|   3|       large|\n",
            "|        Merc 450SLC|15.2|  8|275.8|180|3.07| 3.78| 18.0|  0|  0|   3|   3|       large|\n",
            "| Cadillac Fleetwood|10.4|  8|472.0|205|2.93| 5.25|17.98|  0|  0|   3|   4|       large|\n",
            "|Lincoln Continental|10.4|  8|460.0|215| 3.0|5.424|17.82|  0|  0|   3|   4|       large|\n",
            "|  Chrysler Imperial|14.7|  8|440.0|230|3.23|5.345|17.42|  0|  0|   3|   4|       large|\n",
            "|           Fiat 128|32.4|  4| 78.7| 66|4.08|  2.2|19.47|  1|  1|   4|   1|       small|\n",
            "|        Honda Civic|30.4|  4| 75.7| 52|4.93|1.615|18.52|  1|  1|   4|   2|       small|\n",
            "|     Toyota Corolla|33.9|  4| 71.1| 65|4.22|1.835| 19.9|  1|  1|   4|   1|       small|\n",
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 4\n",
        "@pandas_udf(\"string\")\n",
        "def categorize_cyl(s: pd.Series) -> pd.Series:\n",
        "    return s.apply(lambda x: \"small\" if x <= 4 else \"medium\" if x == 6 else \"large\")\n",
        "\n",
        "spark.udf.register(\"categorize_cylinders\", categorize_cyl)\n",
        "spark.sql(\"SELECT *, categorize_cylinders(cyl) AS car_category FROM cars\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+-------------+\n",
            "|               name| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|is_economical|\n",
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+-------------+\n",
            "|          Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|            0|\n",
            "|      Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|            0|\n",
            "|         Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|            0|\n",
            "|     Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|            0|\n",
            "|  Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|            0|\n",
            "|            Valiant|18.1|  6|225.0|105|2.76| 3.46|20.22|  1|  0|   3|   1|            0|\n",
            "|         Duster 360|14.3|  8|360.0|245|3.21| 3.57|15.84|  0|  0|   3|   4|            0|\n",
            "|          Merc 240D|24.4|  4|146.7| 62|3.69| 3.19| 20.0|  1|  0|   4|   2|            0|\n",
            "|           Merc 230|22.8|  4|140.8| 95|3.92| 3.15| 22.9|  1|  0|   4|   2|            0|\n",
            "|           Merc 280|19.2|  6|167.6|123|3.92| 3.44| 18.3|  1|  0|   4|   4|            0|\n",
            "|          Merc 280C|17.8|  6|167.6|123|3.92| 3.44| 18.9|  1|  0|   4|   4|            0|\n",
            "|         Merc 450SE|16.4|  8|275.8|180|3.07| 4.07| 17.4|  0|  0|   3|   3|            0|\n",
            "|         Merc 450SL|17.3|  8|275.8|180|3.07| 3.73| 17.6|  0|  0|   3|   3|            0|\n",
            "|        Merc 450SLC|15.2|  8|275.8|180|3.07| 3.78| 18.0|  0|  0|   3|   3|            0|\n",
            "| Cadillac Fleetwood|10.4|  8|472.0|205|2.93| 5.25|17.98|  0|  0|   3|   4|            0|\n",
            "|Lincoln Continental|10.4|  8|460.0|215| 3.0|5.424|17.82|  0|  0|   3|   4|            0|\n",
            "|  Chrysler Imperial|14.7|  8|440.0|230|3.23|5.345|17.42|  0|  0|   3|   4|            0|\n",
            "|           Fiat 128|32.4|  4| 78.7| 66|4.08|  2.2|19.47|  1|  1|   4|   1|            1|\n",
            "|        Honda Civic|30.4|  4| 75.7| 52|4.93|1.615|18.52|  1|  1|   4|   2|            1|\n",
            "|     Toyota Corolla|33.9|  4| 71.1| 65|4.22|1.835| 19.9|  1|  1|   4|   1|            1|\n",
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 5\n",
        "@pandas_udf(\"int\")\n",
        "def is_economical(s: pd.Series) -> pd.Series:\n",
        "    return s.apply(lambda x: 1 if x > 25 else 0)\n",
        "\n",
        "spark.udf.register(\"economical\", is_economical)\n",
        "spark.sql(\"SELECT *, economical(mpg) AS is_economical FROM cars\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+----------------+\n",
            "|               name| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|efficiency_score|\n",
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+----------------+\n",
            "|          Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|      0.19090909|\n",
            "|      Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|      0.19090909|\n",
            "|         Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|       0.2451613|\n",
            "|     Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|      0.19454545|\n",
            "|  Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|      0.10685714|\n",
            "|            Valiant|18.1|  6|225.0|105|2.76| 3.46|20.22|  1|  0|   3|   1|      0.17238095|\n",
            "|         Duster 360|14.3|  8|360.0|245|3.21| 3.57|15.84|  0|  0|   3|   4|     0.058367345|\n",
            "|          Merc 240D|24.4|  4|146.7| 62|3.69| 3.19| 20.0|  1|  0|   4|   2|       0.3935484|\n",
            "|           Merc 230|22.8|  4|140.8| 95|3.92| 3.15| 22.9|  1|  0|   4|   2|            0.24|\n",
            "|           Merc 280|19.2|  6|167.6|123|3.92| 3.44| 18.3|  1|  0|   4|   4|      0.15609756|\n",
            "|          Merc 280C|17.8|  6|167.6|123|3.92| 3.44| 18.9|  1|  0|   4|   4|      0.14471544|\n",
            "|         Merc 450SE|16.4|  8|275.8|180|3.07| 4.07| 17.4|  0|  0|   3|   3|      0.09111111|\n",
            "|         Merc 450SL|17.3|  8|275.8|180|3.07| 3.73| 17.6|  0|  0|   3|   3|      0.09611111|\n",
            "|        Merc 450SLC|15.2|  8|275.8|180|3.07| 3.78| 18.0|  0|  0|   3|   3|      0.08444444|\n",
            "| Cadillac Fleetwood|10.4|  8|472.0|205|2.93| 5.25|17.98|  0|  0|   3|   4|     0.050731707|\n",
            "|Lincoln Continental|10.4|  8|460.0|215| 3.0|5.424|17.82|  0|  0|   3|   4|     0.048372094|\n",
            "|  Chrysler Imperial|14.7|  8|440.0|230|3.23|5.345|17.42|  0|  0|   3|   4|      0.06391304|\n",
            "|           Fiat 128|32.4|  4| 78.7| 66|4.08|  2.2|19.47|  1|  1|   4|   1|       0.4909091|\n",
            "|        Honda Civic|30.4|  4| 75.7| 52|4.93|1.615|18.52|  1|  1|   4|   2|       0.5846154|\n",
            "|     Toyota Corolla|33.9|  4| 71.1| 65|4.22|1.835| 19.9|  1|  1|   4|   1|      0.52153844|\n",
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 6\n",
        "@pandas_udf(\"float\")\n",
        "def efficiency_score(s_mpg: pd.Series, s_hp: pd.Series) -> pd.Series:\n",
        "    return s_mpg / s_hp\n",
        "\n",
        "spark.udf.register(\"efficiency_score\", efficiency_score)\n",
        "spark.sql(\"SELECT *, efficiency_score(mpg, hp) AS efficiency_score FROM cars\").show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
